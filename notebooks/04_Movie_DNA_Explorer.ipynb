{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b07cb3",
   "metadata": {},
   "source": [
    "# Project 3: The \"Movie DNA\" Galaxy Explorer\n",
    "## Part 1: Enriching Data with Keywords from TMDB\n",
    "\n",
    "Our goal is to create a rich \"fingerprint\" for each film, and that requires more than just genre and cast. We need to understand the film's plot and themes. In this step, we will use the TMDB API to fetch descriptive keywords and other useful metadata (like poster paths) for every Pre-Code film in our dataset.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Load Data:** Start with our `hollywood_df.pkl` file.\n",
    "2.  **Connect to TMDB:** Use the `tmdbsimple` library and our API key to connect to The Movie Database.\n",
    "3.  **Fetch Keywords:** For each movie (identified by its `tconst`), we will query the TMDB API to find its keywords.\n",
    "4.  **Handle Missing Data:** Some older films may not have entries or keywords. Our code must handle these cases gracefully.\n",
    "5.  **Save Enriched Data:** We will save the result to a new file, `hollywood_df_enriched.pkl`, to use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDB API key loaded successfully from .env file.\n",
      "Using cache directory: ../data/tmdb_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1861e454c5f4b37a7f9d24fa9aea557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Keywords from TMDB (with Cache):   0%|          | 0/4514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tmdbsimple as tmdb\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import json # We need this library for caching\n",
    "\n",
    "# --- 1. Load Environment Variables and Setup TMDB API ---\n",
    "load_dotenv()\n",
    "tmdb.API_KEY = os.getenv('TMDB_API_KEY')\n",
    "if tmdb.API_KEY:\n",
    "    print(\"TMDB API key loaded successfully from .env file.\")\n",
    "else:\n",
    "    print(\"Error: Could not load TMDB_API_KEY from .env file.\")\n",
    "\n",
    "# --- 2. Load Hollywood DataFrame & Setup Cache Directory ---\n",
    "HOLLYWOOD_DF_PATH = \"../data/processed/hollywood_df.pkl\"\n",
    "hollywood_df = pd.read_pickle(HOLLYWOOD_DF_PATH)\n",
    "unique_movies_df = hollywood_df[['tconst', 'primaryTitle', 'startYear']].drop_duplicates(subset=['primaryTitle']).reset_index(drop=True)\n",
    "\n",
    "# Define the directory where we will store our cached results\n",
    "CACHE_DIR = \"../data/tmdb_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "print(f\"Using cache directory: {CACHE_DIR}\")\n",
    "\n",
    "# --- 3. Function to Fetch Keywords (with Caching) ---\n",
    "def get_keywords_from_tmdb(tconst, cache_dir):\n",
    "    \"\"\"\n",
    "    Fetches keywords and poster path for a tconst, using a local file cache.\n",
    "    \"\"\"\n",
    "    cache_filepath = os.path.join(cache_dir, f\"{tconst}.json\")\n",
    "\n",
    "    # First, check if the result is already in our cache\n",
    "    if os.path.exists(cache_filepath):\n",
    "        with open(cache_filepath, 'r') as f:\n",
    "            cached_data = json.load(f)\n",
    "            # Return the cached keywords and poster path\n",
    "            return cached_data.get('keywords', ''), cached_data.get('poster_path', '')\n",
    "\n",
    "    # If not in cache, proceed with the API call\n",
    "    if not tmdb.API_KEY:\n",
    "        return \"NO_API_KEY\", \"\"\n",
    "    try:\n",
    "        find = tmdb.Find(tconst)\n",
    "        response = find.info(external_source='imdb_id')\n",
    "        \n",
    "        if not response['movie_results']:\n",
    "            result_to_cache = {\"status\": \"not_found\", \"keywords\": \"not_found\", \"poster_path\": \"\"}\n",
    "        else:\n",
    "            movie_id = response['movie_results'][0]['id']\n",
    "            movie = tmdb.Movies(movie_id)\n",
    "            keywords = movie.keywords()['keywords']\n",
    "            keyword_str = ' '.join([k['name'] for k in keywords])\n",
    "            poster_path = response['movie_results'][0].get('poster_path', '')\n",
    "            result_to_cache = {\"status\": \"success\", \"keywords\": keyword_str, \"poster_path\": poster_path}\n",
    "\n",
    "    except Exception:\n",
    "        result_to_cache = {\"status\": \"api_error\", \"keywords\": \"api_error\", \"poster_path\": \"\"}\n",
    "\n",
    "    # Save the result to the cache file before returning\n",
    "    with open(cache_filepath, 'w') as f:\n",
    "        json.dump(result_to_cache, f)\n",
    "\n",
    "    return result_to_cache.get('keywords', ''), result_to_cache.get('poster_path', '')\n",
    "\n",
    "# --- 4. Loop Through Movies and Enrich Data ---\n",
    "if tmdb.API_KEY:\n",
    "    tqdm.pandas(desc=\"Fetching Keywords from TMDB (with Cache)\")\n",
    "    # We pass the cache directory to our function using a lambda\n",
    "    results = unique_movies_df['tconst'].progress_apply(lambda tconst: get_keywords_from_tmdb(tconst, CACHE_DIR))\n",
    "    unique_movies_df[['keywords', 'poster_path']] = pd.DataFrame(results.tolist(), index=unique_movies_df.index)\n",
    "\n",
    "    # --- 5. Save the Enriched Data ---\n",
    "    ENRICHED_DF_PATH = \"../data/processed/hollywood_df_enriched.pkl\"\n",
    "    unique_movies_df.to_pickle(ENRICHED_DF_PATH)\n",
    "\n",
    "    print(f\"\\nEnrichment complete. Saved {len(unique_movies_df)} movies with keyword data.\")\n",
    "    print(\"Sample of enriched data:\")\n",
    "    display(unique_movies_df.head())\n",
    "else:\n",
    "    print(\"\\nSkipping data enrichment because TMDB API key was not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30493868",
   "metadata": {},
   "source": [
    "## Part 2: Engineering the \"Movie DNA\" with AI\n",
    "\n",
    "With our enriched dataset, we can now perform the core machine learning task. We will use a pre-trained Sentence Transformer model, a powerful form of NLP AI, to read the plot keywords for each film and convert them into a high-dimensional vector, also known as an \"embedding.\" This vector is the film's unique \"DNA,\" capturing its thematic essence in a way the machine can understand.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Load Enriched Data:** We'll load the `hollywood_df_enriched.pkl` file we created in the previous step.\n",
    "2.  **Instantiate AI Model:** We will load a state-of-the-art model (`all-MiniLM-L6-v2`) from the `sentence-transformers` library. The first time this runs, it will download the model files (a few hundred MB).\n",
    "3.  **Generate Embeddings:** We will feed the `keywords` column into the model. The model will output a 384-dimension vector for each film.\n",
    "4.  **Save the DNA:** We will save these embeddings to a file so we don't have to re-calculate them every time. This is a crucial step in any ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ef694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched movie data loaded successfully.\n",
      "Loading Sentence Transformer model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- 3. Instantiate and Use the Transformer Model ---\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# This model is small but powerful, great for our use case.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# The model will be downloaded from the internet the first time you run this.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Sentence Transformer model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# --- 4. Generate the Embeddings (The \"Movie DNA\") ---\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# The model.encode() function will process the text and output the vectors.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We wrap it in tqdm to see a progress bar.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    300\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    303\u001b[0m     model_name_or_path,\n\u001b[1;32m    304\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    310\u001b[0m         model_name_or_path,\n\u001b[1;32m    311\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    312\u001b[0m         cache_folder\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[1;32m    313\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    314\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    315\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    316\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    317\u001b[0m         tokenizer_kwargs\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[1;32m    318\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    322\u001b[0m         model_name_or_path,\n\u001b[1;32m    323\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    331\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1674\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1674\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1676\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:81\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     84\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:181\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    182\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_only_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4333\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4330\u001b[0m config\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   4332\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[0;32m-> 4333\u001b[0m model_init_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_init_context(is_quantized, _is_ds_init_called)\n\u001b[1;32m   4335\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[1;32m   4336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3736\u001b[0m, in \u001b[0;36mPreTrainedModel.get_init_context\u001b[0;34m(cls, is_quantized, _is_ds_init_called)\u001b[0m\n\u001b[1;32m   3734\u001b[0m         init_contexts\u001b[38;5;241m.\u001b[39mappend(set_quantized_state())\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3736\u001b[0m     init_contexts \u001b[38;5;241m=\u001b[39m [no_init_weights(), init_empty_weights()]\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Load the Enriched Data ---\n",
    "ENRICHED_DF_PATH = \"../data/processed/hollywood_df_enriched.pkl\"\n",
    "enriched_df = pd.read_pickle(ENRICHED_DF_PATH)\n",
    "\n",
    "print(\"Enriched movie data loaded successfully.\")\n",
    "\n",
    "# --- 2. Prepare the Text Data ---\n",
    "# Fill any missing keywords with an empty string so the model can process them.\n",
    "enriched_df['keywords'] = enriched_df['keywords'].fillna('')\n",
    "\n",
    "# Create a list of all keyword strings to feed to the model\n",
    "corpus = enriched_df['keywords'].tolist()\n",
    "\n",
    "# --- 3. Instantiate and Use the Transformer Model ---\n",
    "# This model is small but powerful, great for our use case.\n",
    "# The model will be downloaded from the internet the first time you run this.\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- 4. Generate the Embeddings (The \"Movie DNA\") ---\n",
    "# The model.encode() function will process the text and output the vectors.\n",
    "# We wrap it in tqdm to see a progress bar.\n",
    "print(\"Generating movie DNA embeddings... (This may take a minute)\")\n",
    "movie_dna_embeddings = model.encode(corpus, show_progress_bar=True)\n",
    "\n",
    "# --- 5. Save the Embeddings ---\n",
    "EMBEDDINGS_PATH = \"../data/processed/movie_dna_embeddings.npy\"\n",
    "np.save(EMBEDDINGS_PATH, movie_dna_embeddings)\n",
    "\n",
    "print(\"\\nMovie DNA creation complete!\")\n",
    "print(f\"Shape of our DNA matrix: {movie_dna_embeddings.shape}\")\n",
    "print(f\"(This means {movie_dna_embeddings.shape[0]} movies, each with a {movie_dna_embeddings.shape[1]}-dimension DNA vector)\")\n",
    "print(f\"Embeddings saved to: {EMBEDDINGS_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
