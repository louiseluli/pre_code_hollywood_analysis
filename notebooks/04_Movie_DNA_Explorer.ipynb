{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b07cb3",
   "metadata": {},
   "source": [
    "# Project 3: The \"Movie DNA\" Galaxy Explorer\n",
    "## Part 1: Enriching Data with Keywords from TMDB\n",
    "\n",
    "Our goal is to create a rich \"fingerprint\" for each film, and that requires more than just genre and cast. We need to understand the film's plot and themes. In this step, we will use the TMDB API to fetch descriptive keywords and other useful metadata (like poster paths) for every Pre-Code film in our dataset.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Load Data:** Start with our `hollywood_df.pkl` file.\n",
    "2.  **Connect to TMDB:** Use the `tmdbsimple` library and our API key to connect to The Movie Database.\n",
    "3.  **Fetch Keywords:** For each movie (identified by its `tconst`), we will query the TMDB API to find its keywords.\n",
    "4.  **Handle Missing Data:** Some older films may not have entries or keywords. Our code must handle these cases gracefully.\n",
    "5.  **Save Enriched Data:** We will save the result to a new file, `hollywood_df_enriched.pkl`, to use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8518afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDB API key loaded successfully from .env file.\n",
      "Using cache directory: ../data/tmdb_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1861e454c5f4b37a7f9d24fa9aea557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Keywords from TMDB (with Cache):   0%|          | 0/4514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enrichment complete. Saved 4514 movies with keyword data.\n",
      "Sample of enriched data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>keywords</th>\n",
       "      <th>poster_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0017578</td>\n",
       "      <td>The Wrecker</td>\n",
       "      <td>1929</td>\n",
       "      <td></td>\n",
       "      <td>/oGCsBdjxDq7b4eTpTsjJrA6VayX.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0018362</td>\n",
       "      <td>The Scar of Shame</td>\n",
       "      <td>1929</td>\n",
       "      <td>marriage contract prison escape class differen...</td>\n",
       "      <td>/tosJ21bDxJzvg2JcTuKQMqWglvM.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0018588</td>\n",
       "      <td>Three Loves</td>\n",
       "      <td>1929</td>\n",
       "      <td>black and white silent film</td>\n",
       "      <td>/ncyxOfdoS0Rz9VWHxyx6HLyU5nB.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0018630</td>\n",
       "      <td>After the Verdict</td>\n",
       "      <td>1929</td>\n",
       "      <td>sports</td>\n",
       "      <td>/dOGxHjXBFp1D59hZLzBl9Gheg20.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0018685</td>\n",
       "      <td>The Bellamy Trial</td>\n",
       "      <td>1929</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst       primaryTitle  startYear  \\\n",
       "0  tt0017578        The Wrecker       1929   \n",
       "1  tt0018362  The Scar of Shame       1929   \n",
       "2  tt0018588        Three Loves       1929   \n",
       "3  tt0018630  After the Verdict       1929   \n",
       "4  tt0018685  The Bellamy Trial       1929   \n",
       "\n",
       "                                            keywords  \\\n",
       "0                                                      \n",
       "1  marriage contract prison escape class differen...   \n",
       "2                        black and white silent film   \n",
       "3                                             sports   \n",
       "4                                                      \n",
       "\n",
       "                        poster_path  \n",
       "0  /oGCsBdjxDq7b4eTpTsjJrA6VayX.jpg  \n",
       "1  /tosJ21bDxJzvg2JcTuKQMqWglvM.jpg  \n",
       "2  /ncyxOfdoS0Rz9VWHxyx6HLyU5nB.jpg  \n",
       "3  /dOGxHjXBFp1D59hZLzBl9Gheg20.jpg  \n",
       "4                              None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tmdbsimple as tmdb\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import json # We need this library for caching\n",
    "\n",
    "# --- 1. Load Environment Variables and Setup TMDB API ---\n",
    "load_dotenv()\n",
    "tmdb.API_KEY = os.getenv('TMDB_API_KEY')\n",
    "if tmdb.API_KEY:\n",
    "    print(\"TMDB API key loaded successfully from .env file.\")\n",
    "else:\n",
    "    print(\"Error: Could not load TMDB_API_KEY from .env file.\")\n",
    "\n",
    "# --- 2. Load Hollywood DataFrame & Setup Cache Directory ---\n",
    "HOLLYWOOD_DF_PATH = \"../data/processed/hollywood_df.pkl\"\n",
    "hollywood_df = pd.read_pickle(HOLLYWOOD_DF_PATH)\n",
    "unique_movies_df = hollywood_df[['tconst', 'primaryTitle', 'startYear']].drop_duplicates(subset=['primaryTitle']).reset_index(drop=True)\n",
    "\n",
    "# Define the directory where we will store our cached results\n",
    "CACHE_DIR = \"../data/tmdb_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "print(f\"Using cache directory: {CACHE_DIR}\")\n",
    "\n",
    "# --- 3. Function to Fetch Keywords (with Caching) ---\n",
    "def get_keywords_from_tmdb(tconst, cache_dir):\n",
    "    \"\"\"\n",
    "    Fetches keywords and poster path for a tconst, using a local file cache.\n",
    "    \"\"\"\n",
    "    cache_filepath = os.path.join(cache_dir, f\"{tconst}.json\")\n",
    "\n",
    "    # First, check if the result is already in our cache\n",
    "    if os.path.exists(cache_filepath):\n",
    "        with open(cache_filepath, 'r') as f:\n",
    "            cached_data = json.load(f)\n",
    "            # Return the cached keywords and poster path\n",
    "            return cached_data.get('keywords', ''), cached_data.get('poster_path', '')\n",
    "\n",
    "    # If not in cache, proceed with the API call\n",
    "    if not tmdb.API_KEY:\n",
    "        return \"NO_API_KEY\", \"\"\n",
    "    try:\n",
    "        find = tmdb.Find(tconst)\n",
    "        response = find.info(external_source='imdb_id')\n",
    "        \n",
    "        if not response['movie_results']:\n",
    "            result_to_cache = {\"status\": \"not_found\", \"keywords\": \"not_found\", \"poster_path\": \"\"}\n",
    "        else:\n",
    "            movie_id = response['movie_results'][0]['id']\n",
    "            movie = tmdb.Movies(movie_id)\n",
    "            keywords = movie.keywords()['keywords']\n",
    "            keyword_str = ' '.join([k['name'] for k in keywords])\n",
    "            poster_path = response['movie_results'][0].get('poster_path', '')\n",
    "            result_to_cache = {\"status\": \"success\", \"keywords\": keyword_str, \"poster_path\": poster_path}\n",
    "\n",
    "    except Exception:\n",
    "        result_to_cache = {\"status\": \"api_error\", \"keywords\": \"api_error\", \"poster_path\": \"\"}\n",
    "\n",
    "    # Save the result to the cache file before returning\n",
    "    with open(cache_filepath, 'w') as f:\n",
    "        json.dump(result_to_cache, f)\n",
    "\n",
    "    return result_to_cache.get('keywords', ''), result_to_cache.get('poster_path', '')\n",
    "\n",
    "# --- 4. Loop Through Movies and Enrich Data ---\n",
    "if tmdb.API_KEY:\n",
    "    tqdm.pandas(desc=\"Fetching Keywords from TMDB (with Cache)\")\n",
    "    # We pass the cache directory to our function using a lambda\n",
    "    results = unique_movies_df['tconst'].progress_apply(lambda tconst: get_keywords_from_tmdb(tconst, CACHE_DIR))\n",
    "    unique_movies_df[['keywords', 'poster_path']] = pd.DataFrame(results.tolist(), index=unique_movies_df.index)\n",
    "\n",
    "    # --- 5. Save the Enriched Data ---\n",
    "    ENRICHED_DF_PATH = \"../data/processed/hollywood_df_enriched.pkl\"\n",
    "    unique_movies_df.to_pickle(ENRICHED_DF_PATH)\n",
    "\n",
    "    print(f\"\\nEnrichment complete. Saved {len(unique_movies_df)} movies with keyword data.\")\n",
    "    print(\"Sample of enriched data:\")\n",
    "    display(unique_movies_df.head())\n",
    "else:\n",
    "    print(\"\\nSkipping data enrichment because TMDB API key was not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30493868",
   "metadata": {},
   "source": [
    "## Part 2: Engineering the \"Movie DNA\" with AI\n",
    "\n",
    "With our enriched dataset, we can now perform the core machine learning task. We will use a pre-trained Sentence Transformer model, a powerful form of NLP AI, to read the plot keywords for each film and convert them into a high-dimensional vector, also known as an \"embedding.\" This vector is the film's unique \"DNA,\" capturing its thematic essence in a way the machine can understand.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Load Enriched Data:** We'll load the `hollywood_df_enriched.pkl` file we created in the previous step.\n",
    "2.  **Instantiate AI Model:** We will load a state-of-the-art model (`all-MiniLM-L6-v2`) from the `sentence-transformers` library. The first time this runs, it will download the model files (a few hundred MB).\n",
    "3.  **Generate Embeddings:** We will feed the `keywords` column into the model. The model will output a 384-dimension vector for each film.\n",
    "4.  **Save the DNA:** We will save these embeddings to a file so we don't have to re-calculate them every time. This is a crucial step in any ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ef694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched movie data loaded successfully.\n",
      "No cached embeddings found. Generating new ones...\n",
      "Loading Sentence Transformer model (this may download the model)...\n",
      "Model loaded.\n",
      "Generating movie DNA embeddings... (This may take a minute)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da48b57107694162a02bfab91aaafa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to cache: ../data/processed/movie_dna_embeddings.npy\n",
      "\n",
      "Movie DNA creation complete!\n",
      "Shape of our DNA matrix: (4514, 384)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Define Paths and Load Enriched Data ---\n",
    "ENRICHED_DF_PATH = \"../data/processed/hollywood_df_enriched.pkl\"\n",
    "EMBEDDINGS_PATH = \"../data/processed/movie_dna_embeddings.npy\"\n",
    "\n",
    "enriched_df = pd.read_pickle(ENRICHED_DF_PATH)\n",
    "print(\"Enriched movie data loaded successfully.\")\n",
    "\n",
    "# --- 2. Check for Cached Embeddings ---\n",
    "if os.path.exists(EMBEDDINGS_PATH):\n",
    "    print(f\"Found cached 'Movie DNA' embeddings. Loading from: {EMBEDDINGS_PATH}\")\n",
    "    movie_dna_embeddings = np.load(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    print(\"No cached embeddings found. Generating new ones...\")\n",
    "    \n",
    "    # --- Prepare the Text Data ---\n",
    "    enriched_df['keywords'] = enriched_df['keywords'].fillna('')\n",
    "    corpus = enriched_df['keywords'].tolist()\n",
    "\n",
    "    # --- Instantiate and Use the Transformer Model ---\n",
    "    print(\"Loading Sentence Transformer model (this may download the model)...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # --- Generate the Embeddings (The \"Movie DNA\") ---\n",
    "    print(\"Generating movie DNA embeddings... (This may take a minute)\")\n",
    "    movie_dna_embeddings = model.encode(corpus, show_progress_bar=True)\n",
    "\n",
    "    # --- Save the Embeddings to the Cache ---\n",
    "    np.save(EMBEDDINGS_PATH, movie_dna_embeddings)\n",
    "    print(f\"Embeddings saved to cache: {EMBEDDINGS_PATH}\")\n",
    "\n",
    "print(\"\\nMovie DNA creation complete!\")\n",
    "print(f\"Shape of our DNA matrix: {movie_dna_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eabba1",
   "metadata": {},
   "source": [
    "## Part 3: Mapping the Galaxy with t-SNE\n",
    "\n",
    "Our movie DNA is in a 384-dimensional space, which is impossible to visualize directly. In this step, we will use **t-SNE**, a powerful dimensionality reduction algorithm, to project these high-dimensional vectors down to a 2D space (an 'x' and 'y' coordinate for each movie). This creates a map where thematically similar movies are positioned close together, forming a \"galaxy\" of clusters we can explore.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Load Data:** We will load our cached `movie_dna_embeddings.npy` file.\n",
    "2.  **Instantiate t-SNE:** We will configure the t-SNE model from `scikit-learn`.\n",
    "3.  **Run Reduction:** We'll apply the `fit_transform` method to our embeddings. This is a computationally intensive step.\n",
    "4.  **Cache Results:** Just like before, we will save the resulting 2D coordinates to a file to avoid re-running this expensive step.\n",
    "5.  **Merge and Save:** We will merge these new 'x' and 'y' coordinates back into our main movie DataFrame for the final visualization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854bb4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cached t-SNE coordinates found. Running dimensionality reduction...\n",
      "This is computationally intensive and will take several minutes.\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 4514 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 4514 samples in 0.197s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louisesfer/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4514\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4514\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4514\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4514\n",
      "[t-SNE] Computed conditional probabilities for sample 4514 / 4514\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 64.495758\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.973515\n",
      "t-SNE coordinates saved to cache: ../data/processed/tsne_2d_coordinates.npy\n",
      "\n",
      "Merging 2D coordinates into the main movie DataFrame...\n",
      "Final DataFrame for visualization saved to: ../data/processed/hollywood_galaxy_df.pkl\n",
      "Sample of the final data with 'x' and 'y' coordinates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>keywords</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0017578</td>\n",
       "      <td>The Wrecker</td>\n",
       "      <td>1929</td>\n",
       "      <td></td>\n",
       "      <td>/oGCsBdjxDq7b4eTpTsjJrA6VayX.jpg</td>\n",
       "      <td>2.520578</td>\n",
       "      <td>35.050861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0018362</td>\n",
       "      <td>The Scar of Shame</td>\n",
       "      <td>1929</td>\n",
       "      <td>marriage contract prison escape class differen...</td>\n",
       "      <td>/tosJ21bDxJzvg2JcTuKQMqWglvM.jpg</td>\n",
       "      <td>-9.107474</td>\n",
       "      <td>-16.665449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0018588</td>\n",
       "      <td>Three Loves</td>\n",
       "      <td>1929</td>\n",
       "      <td>black and white silent film</td>\n",
       "      <td>/ncyxOfdoS0Rz9VWHxyx6HLyU5nB.jpg</td>\n",
       "      <td>31.801432</td>\n",
       "      <td>-39.114357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0018630</td>\n",
       "      <td>After the Verdict</td>\n",
       "      <td>1929</td>\n",
       "      <td>sports</td>\n",
       "      <td>/dOGxHjXBFp1D59hZLzBl9Gheg20.jpg</td>\n",
       "      <td>11.879026</td>\n",
       "      <td>0.767587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0018685</td>\n",
       "      <td>The Bellamy Trial</td>\n",
       "      <td>1929</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-2.034620</td>\n",
       "      <td>34.347107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst       primaryTitle  startYear  \\\n",
       "0  tt0017578        The Wrecker       1929   \n",
       "1  tt0018362  The Scar of Shame       1929   \n",
       "2  tt0018588        Three Loves       1929   \n",
       "3  tt0018630  After the Verdict       1929   \n",
       "4  tt0018685  The Bellamy Trial       1929   \n",
       "\n",
       "                                            keywords  \\\n",
       "0                                                      \n",
       "1  marriage contract prison escape class differen...   \n",
       "2                        black and white silent film   \n",
       "3                                             sports   \n",
       "4                                                      \n",
       "\n",
       "                        poster_path          x          y  \n",
       "0  /oGCsBdjxDq7b4eTpTsjJrA6VayX.jpg   2.520578  35.050861  \n",
       "1  /tosJ21bDxJzvg2JcTuKQMqWglvM.jpg  -9.107474 -16.665449  \n",
       "2  /ncyxOfdoS0Rz9VWHxyx6HLyU5nB.jpg  31.801432 -39.114357  \n",
       "3  /dOGxHjXBFp1D59hZLzBl9Gheg20.jpg  11.879026   0.767587  \n",
       "4                              None  -2.034620  34.347107  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "# --- 1. Define Paths and Check for Cached Coordinates ---\n",
    "ENRICHED_DF_PATH = \"../data/processed/hollywood_df_enriched.pkl\"\n",
    "EMBEDDINGS_PATH = \"../data/processed/movie_dna_embeddings.npy\"\n",
    "TSNE_COORDS_PATH = \"../data/processed/tsne_2d_coordinates.npy\"\n",
    "\n",
    "# Load the data we'll need\n",
    "enriched_df = pd.read_pickle(ENRICHED_DF_PATH)\n",
    "movie_dna_embeddings = np.load(EMBEDDINGS_PATH)\n",
    "\n",
    "# --- 2. Run t-SNE or Load from Cache ---\n",
    "if os.path.exists(TSNE_COORDS_PATH):\n",
    "    print(f\"Found cached t-SNE coordinates. Loading from: {TSNE_COORDS_PATH}\")\n",
    "    tsne_coords = np.load(TSNE_COORDS_PATH)\n",
    "else:\n",
    "    print(\"No cached t-SNE coordinates found. Running dimensionality reduction...\")\n",
    "    print(\"This is computationally intensive and will take several minutes.\")\n",
    "    \n",
    "    # Configure the t-SNE model\n",
    "    tsne = TSNE(\n",
    "        n_components=2,          # We want a 2D map\n",
    "        perplexity=30,           # A standard value for this parameter\n",
    "        init='pca',              # Initialize with PCA for better stability\n",
    "        n_iter=1000,             # Number of iterations\n",
    "        random_state=42,         # For reproducible results\n",
    "        verbose=1                # To see the progress\n",
    "    )\n",
    "    \n",
    "    # Run the model\n",
    "    tsne_coords = tsne.fit_transform(movie_dna_embeddings)\n",
    "    \n",
    "    # Save the results to our cache\n",
    "    np.save(TSNE_COORDS_PATH, tsne_coords)\n",
    "    print(f\"t-SNE coordinates saved to cache: {TSNE_COORDS_PATH}\")\n",
    "\n",
    "# --- 3. Merge Coordinates into our Main DataFrame ---\n",
    "print(\"\\nMerging 2D coordinates into the main movie DataFrame...\")\n",
    "\n",
    "# Create a DataFrame from our 2D coordinates\n",
    "coords_df = pd.DataFrame(tsne_coords, columns=['x', 'y'])\n",
    "\n",
    "# Merge it with our original enriched data\n",
    "final_galaxy_df = pd.concat([enriched_df.reset_index(drop=True), coords_df], axis=1)\n",
    "\n",
    "# Save this final, fully-processed DataFrame\n",
    "FINAL_DF_PATH = \"../data/processed/hollywood_galaxy_df.pkl\"\n",
    "final_galaxy_df.to_pickle(FINAL_DF_PATH)\n",
    "\n",
    "print(f\"Final DataFrame for visualization saved to: {FINAL_DF_PATH}\")\n",
    "print(\"Sample of the final data with 'x' and 'y' coordinates:\")\n",
    "display(final_galaxy_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
